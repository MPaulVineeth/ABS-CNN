import HighQualityVisualization from "@/components/high-quality-visualizations"

export default function AttentionMechanism() {
  return (
    <div className="space-y-8">
      <div className="grid grid-cols-1 md:grid-cols-2 gap-8">
        <div>
          <h3 className="text-lg font-semibold mb-4">Convolutional Block Attention Module (CBAM)</h3>
          <p className="text-muted-foreground mb-4">
            The attention mechanism used in ABS-CNN is the Convolutional Block Attention Module (CBAM), which consists
            of two complementary modules: channel attention and spatial attention.
          </p>
          <p className="text-muted-foreground mb-4">
            CBAM guides the CNN model to focus on important features and suppress nonessential features, improving the
            model's ability to detect abnormal traffic.
          </p>
          <h4 className="font-medium mt-6 mb-2">Channel Attention</h4>
          <p className="text-muted-foreground mb-4">
            Channel attention assigns more weight to channels with more or distinct features, while assigning the least
            weight to channels with the least or insignificant features.
          </p>
          <h4 className="font-medium mt-6 mb-2">Spatial Attention</h4>
          <p className="text-muted-foreground">
            Spatial attention uses the spatial relationships of the original features to help the network localize to
            the locations of the generated fine features. It focuses on where the important features are in the image.
          </p>
        </div>
        <div className="flex items-center justify-center">
          <div className="w-full max-w-md">
            <HighQualityVisualization type="attention" />
            <p className="text-sm text-muted-foreground text-center mt-2">Figure: CBAM Architecture</p>
          </div>
        </div>
      </div>

      <div>
        <h3 className="text-lg font-semibold mb-4">Mathematical Formulation</h3>
        <div className="bg-muted p-4 rounded-lg">
          <p className="mb-2">
            The image F is processed under the channel attention to obtain the channel attention map Mc:
          </p>
          <p className="font-mono mb-4">Mc(F) = sigmoid(PMLP(Pavg(F)) + PMLP(Pmax(F)))</p>

          <p className="mb-2">The spatial attention map Ms is then obtained:</p>
          <p className="font-mono mb-4">Ms(F) = sigmoid(f7×7([Fsavg; Fsmax]))</p>

          <p className="mb-2">The final feature map F'' is generated by:</p>
          <p className="font-mono">F' = Mc(F) ⊗ F</p>
          <p className="font-mono">F'' = Ms(F') ⊗ F'</p>
        </div>
      </div>

      <div>
        <h3 className="text-lg font-semibold mb-4">Benefits in Abnormal Traffic Detection</h3>
        <ul className="space-y-2">
          <li className="flex items-start">
            <span className="font-medium mr-2">•</span>
            <span>Enhances the differentiation of similar traffic features</span>
          </li>
          <li className="flex items-start">
            <span className="font-medium mr-2">•</span>
            <span>Improves the model's ability to focus on important traffic characteristics</span>
          </li>
          <li className="flex items-start">
            <span className="font-medium mr-2">•</span>
            <span>Increases detection accuracy for malicious traffic patterns</span>
          </li>
          <li className="flex items-start">
            <span className="font-medium mr-2">•</span>
            <span>Reduces false positives by suppressing nonessential features</span>
          </li>
        </ul>
      </div>
    </div>
  )
}

